[{"title":"VMware Fusion虚拟机配置固定IP","date":"2017-03-05T08:48:28.000Z","path":"VMware Fusion虚拟机配置固定IP/","text":"现在做开发一般都不是在本机开发，一般是在虚拟机或者docker中配置开发环境，代码同步到虚拟机中进行调试，这样既能够保证开发环境和线上环境能够更加相同，而且也能够使本地足够的干净，不至于被大量的开发组件搞得焦头烂额，这次我们要说的就是在mac上使用VMware Fusion配置虚拟机的静态IP 关闭VMware Fusion关闭DHCP因为要设置静态的固定IP，自然首先要关掉DHCP，但是由于mac上的VMware Fusion并没有可视化的界面去配置网络，因此只能通过直接修改配置文件来修改 进入配置目录 1$ sudo vim /Library/Preferences/VMware\\ Fusion/networking 其中有一行VNET_8_DHCP yes```，将yes改成no就可以关掉DHCP了。12345678910## 查看网关和DNS现在要查看下我们要设置的静态ip的网关和DNS打开文件``` bashvim /Library/Preferences/VMware\\ Fusion/vmnet8/dhcpd.conf 其中 range就是可选的静态ip的范围 option broadcast-address是广播地址，也是我们的默认网关地址 option domain-name-servers就是DNS地址 我们之后在虚拟机里设置ip和网关和dns就要根据这个进行设置了 centos6.7的虚拟机配置接下来就要设置虚拟机中的静态IP 涉及到的两个文件 /etc/sysconfig/network /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/sysconfig/network设置网关GATEWAY=默认网关 /etc/sysconfig/network-scripts/ifcfg-eth0123456789101112DEVICE=&quot;eth0&quot; BOOTPROTO=&quot;static&quot; IPADDR=192.168.110.129 // 之前的IP范围中任选NETMASK=255.255.255.0 HWADDR=&quot;00:0C:29:53:A8:1D&quot; IPV6INIT=&quot;no&quot; NM_CONTROLLED=&quot;yes&quot; ONBOOT=&quot;yes&quot; TYPE=&quot;Ethernet&quot; UUID=&quot;f933b2bf-47eb-42f3-bea9-1f54088a2cb7&quot; DNS1=192.168.110.2 // DNSGATEWAY=192.168.110.255 // 默认网关 这样配置后重启就会生效，虚拟机的ip就会固定，可以直接在宿主机上ping虚拟机了 不过正常开发我们一般虚拟机都在后台运行，需要进入虚拟机配置的时候直接在终端ssh，因此为了方便可以设置使用秘钥免密登录服务器，具体可以参考这边篇文章ssh public key认证免密码登录 参考文章: Mac VMware fusion 专用网络关闭DHCP VMware Fusion Nat链接方式下固定IP VMware虚拟机中CentOS 6.4设置固定IP","tags":[{"name":"VMware","slug":"VMware","permalink":"http://yunxuan.site/tags/VMware/"},{"name":"Centos","slug":"Centos","permalink":"http://yunxuan.site/tags/Centos/"},{"name":"固定IP","slug":"固定IP","permalink":"http://yunxuan.site/tags/固定IP/"}]},{"title":"Memcached详解|与Redis对比","date":"2017-01-06T06:39:56.000Z","path":"memcache详解|与Redis对比/","text":"在说Memcached内存分配策略之前，先说一下Memcache使用的Slab Allocator内存分配机制。 Slab allocator：为了避免内存内碎片的问题，按照预先规定的大小，将分配的内存分割成特定长度的块。页框中存储着小于当前页框大小的数据，而且页框是不会销毁的，释放后可以通过申请再次使用。 Memcached内存分配机制Memcache内存分配Memcached存储有三个概念需要搞清楚，分别是page，slab和chunk: page对应的实际的物理空间，page的默认大小是1M，可以在启动的时候通过-I参数修改 chunk是固定大小的内存空间，默认大小为96Byte 同样大小的chunk被称为slab Memcache启动时通过-m参数可以设置大小，-f参数是slab的增长因子。slab依靠增长因子计算出slab的序列，默认-f的值是1.25，slab2的大小是1.25*slab1，呈现阶梯式的增长。每个page中都会这样分配，page一旦被分配在重启前不会被回收或者重新分配。而整个-m定义的大小空间又被这样分配，不过总的大小空间是随着数据的存入才会逐渐进行分配使用的。 如果一个新的缓存数据要被存放，memcached首先选择一个合适的slab，然后查看该slab是否还有空闲的chunk，如果有则直接存放进去，如果没有则要进行申请。slab申请内存时以page为单位，所以在放入第一个数据时无论大小为多少，都会有1M大小的page被分配给该slab。申请到page后，slab会将这个page的内存按chunk的大小进行切分，这样就变成了一个chunk的数组，再从这个chunk数组中选择一个用于存储数据。 惰性释放策略memcached中过期的数据不会自动清除。当记录失效之后，仅仅是对于访问的客户端透明。但是memcached本身也不会将过期的数据删除，因此不会在监视过期数据上耗费CPU，这种就是惰性释放策略。而memcached在存储时，如果已满，则采用Least Recently Used（LRU）策略，来清除最近最少使用的数据。 与Redis的对比数据类型Redis的数据类型比较比较多，有Hash，Set，List，zSet等，使用场景更加丰富，Memcached只有key-value类型。 事务性Redis可以通过MULTI,EXEC等命令来支持事务，而memcached除了increment/decrement这样的原子操作命令，不存在对事务的支持。 数据持久化memcached是不保证数据的有效性的，基于LRU的淘汰策略会使得部分数据丢失，，但是有许多基于memcached协议的项目实现了数据的持久化，例如memcache DB使用BerkeleyDB进行数据存储，但本质上它已经不是一个cache server，而只是一个兼容memcached的协议key-valueData Store了 Redis自持主从，内建支持两种持久化方案，snapshot快照和AOF增量Log方式。快照顾名思义就是隔一段时间将完整的数据dump下来存储在文件中。AOF增量Log则是记录对数据的修改操作（实际上记录的就是每个对数据产生修改的命令本身），两种方案可以并存，也各有优缺点，具体参见Redis Persistence 以上Redis的数据备份持久化方案等，如果不需要，为了提高性能，也完全可以Disable 性能Memcached自身并不主动定期检查和标记哪些数据需要被淘汰，只有当再次读取相关数据时才检查时间戳，或者当内存不够使用需要主动淘汰数据时进一步检查LRU数据。 Redis为了减少大量小数据CMD操作的网络通讯时间开销 RTT (Round Trip Time)，支持pipeline和script技术。 所谓的pipeline就是支持在一次通讯中，发送多个命令给服务器批量执行，带来的代价是服务器端需要更多的内存来缓存查询结果。 Redis内嵌了LUA解析器，可以执行lua脚本，脚本可以通过eval等命令直接执行，也可以使用script load等方式上传到服务器端的script cache中重复使用。 这两种方式都可以有效地减少网络通讯开销，增加数据吞吐率。 对于KV的操作，memcached和Redis都支持multiple的get和set命令（memcached的multiple set命令貌似只在二进制的协议中支持），这同样有利于性能的提升。 实际性能方面，网上有很多测试比较，给出的结果各不相同，这无疑和各种测试的测试用例，测试环境，和测试时具体使用的客户端Library实现有关。但是总体看下来，比较靠谱的结论是在kv类操作上，两者的性能接近，memcached的结构更加简单，理论上应该会略微快一些。 对比总结Redis的作者Salvatore Sanfilippo曾经对这两种基于内存的数据存储系统进行过比较，总体来看还是比较客观的，现总结如下： 性能对比：由于Redis只使用单核，而memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比memcached性能更高。而在100k以上的数据中，memcached性能要高于Redis，虽然Redis最近也在存储大数据的性能上进行优化，但是比起memcached，还是稍有逊色。 内存使用效率对比：使用简单的key-value存储的话，memcached的内存利用率更高，而如果Redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于memcached。另外，memcached使用预分配的内存池的方式，带来一定程度的空间浪费 并且在内存仍然有很大空间时，新的数据也可能会被剔除，而Redis使用现场申请内存的方式来存储数据，不会剔除任何非临时数据 Redis更适合作为存储而不是cache。 Redis支持服务器端的数据操作：Redis相比memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，通常在memcached里，你需要将数据拿到客户端来进行类似的修改再set回去。这大大增加了网络IO的次数和数据体积。在Redis中，这些复杂的操作通常和一般的GET/SET一样高效。所以，如果需要缓存能够支持更复杂的结构和操作，那么Redis会是不错的选择。 另外，贴一些前辈们使用Redis的经验和教训： 要进行master-slave配置，出现服务故障时可以支持切换。 在master侧禁用数据持久化，只需在slave上配置数据持久化。 物理内存+虚拟内存不足，这个时候dump一直死着，时间久了机器挂掉。这个情况就是灾难。 当Redis物理内存使用超过内存总容量的3/5时就会开始比较危险了，就开始做swap,内存碎片大。 当达到最大内存时，会清空带有过期时间的key，即使key未到过期时间。 Redis与DB同步写的问题，先写DB，后写Redis，因为写内存基本上没有问题。 需要注意的点 memcache分配给某个slab的内存页不能再分配给其他slab。 flush_all不能重置memcache分配内存页的格局，只是给所有的item置为过期。 启动memcached时可以通过-M参数禁止LRU替换，在内存用尽时add和set会返回失败 由于memcache的分布式是客户端程序通过hash算法得到的key取模来实现，不同的语言可能会采用不同的hash算法，同样的客户端程序也有可能使用相异的方法，因此在多语言、多模块共用同一组memcached服务时，一定要注意在客户端选择相同的hash算法（之前有过php写入MC但是用go直接读取MC读到的数据无法解析） memcached启动时指定的是数据存储量，没有包括本身占用的内存、以及为了保存数据而设置的管理空间。因此它占用的内存量会多于启动时指定的内存分配量，这点需要注意。 memcache存储的时候对key的长度有限制，php和C的最大长度都是250 参考文章: memcached Redis 对比分析 Memcached内存分配策略 Memcache内存分配 | Memcache存储策略","tags":[{"name":"Memcached","slug":"Memcached","permalink":"http://yunxuan.site/tags/Memcached/"},{"name":"缓存","slug":"缓存","permalink":"http://yunxuan.site/tags/缓存/"},{"name":"Redis","slug":"Redis","permalink":"http://yunxuan.site/tags/Redis/"}]},{"title":"斗志消磨慢点吧","date":"2016-11-21T08:26:04.000Z","path":"斗志消磨慢点吧/","text":"这篇小记写于2016/09/20，不过当时没有带自己电脑，就写在了云笔记上，现在把这篇誊到博客上来。 从贝壳三岁生日博饼会上回来，又和uber的司机聊了一路，想写点什么。 首先还是我贝壳已经那么棒，应该说还在越来越棒，我其实心里挺感动的，看着贝壳一步步走来，长大。当时把贝壳交给他们的时候真的是万分不舍，就像看着自己的孩子交给别人一样。不过现在看来，哪里是交给别人，贝壳就是这样成长起来的，想着我们能把贝壳交到他们手里真的是太好了，也许上一届也会这么想？大概吧，哈哈哈。 再来说说回来的路上司机讲的一些东西。 司机师傅人很好，聊了很多，尤其是一开始他听说我是个应届毕业生，就开始跟我讲他毕业的时候的事情。 他说他一毕业，就去了广州，去了一家普通的公司。他说刚毕业的学生都会有一股冲进，想要做点什么，想要学到东西，这种态度是非常宝贵的，他说他刚进公司就有一摊事情要忙，可是他又什么也不会，就没办法，就要学，他说每天过的像狗一样，但是现在回想起来，那时候的他是成长最快的时候。他说一个应届生，最重要的就是这种品质，你刚毕业，技术不如人家，有没有人家有激情，公司凭什么要你。 他说进入社会的人， 会被慢慢地磨平棱角，刚毕业的学生，在意的是得，不会很在意失，他们想的更多的是我学到了什么，我得到了什么，不会去过多衡量自己付出了多少，这样也就让他们不回去害怕付出，为了达到目标而不断努力，可是在社会里呆的时间越长，人们会越来越计较得失，得到的失去的之间比较，赚不赚。 他说他在广州的几年真的是好苦，没人带，一切都要自己想办法来学习，工作就摆在那里，不会就要学，就要完成工作。过的非常凄惨，但是他说他这辈子最后悔的事情大概就是没有坚持留在广州，他放弃了，几年之后，他回到了老家。他说他真的很后悔，他说他感觉自己就像一个逃兵，为了躲避那种生活而回到了老家，他说他离开前一直觉着广州就是个地狱，人活着像狗，可是他说他回了老家，才想明白，那不是地狱，那是炼狱，浴火才能重生，经受得起考验才能更强。 他说他试着想回去，但是他做不到了，他从一个快节奏的城市回到了慢节奏的城市，他就再也回不去了，他受不了那种生活了，他说他最害怕听见哪个应届生变得没有冲进，安逸的活着，也许本身追求安逸并没有错，但同时你也放弃了自己的未来。 记下这些，不知道是有感而发还是什么，也许以后会去其他城市，加入到快节奏的生活中，大概我心中想的就是，希望记下这些能让我的斗志消磨的更慢一点吧。感谢那位司机师傅，真的非常感谢，对我一个不相干的人说了这么多，也许我并不能因为他的话有多大改变，但至少，我会有了改变的想法吧，感谢他，那位不知名的uber司机。","tags":[{"name":"生活小记","slug":"生活小记","permalink":"http://yunxuan.site/tags/生活小记/"},{"name":"贝壳","slug":"贝壳","permalink":"http://yunxuan.site/tags/贝壳/"}]},{"title":"新兵训练之Feed架构设计（二）","date":"2016-10-03T10:05:51.000Z","path":"新兵训练之Feed架构设计（二）/","text":"这是这个系列的第二篇，最近还是太懒了，拖了好久。本篇主要是分析一下Feed架构的业务逻辑，和三种方式的设计。首先是Feed业务中的五个核心操作 ： weibo/create weibo/delete follow/create follow/delete user/feedline ，两种模式：拉模式 推模式。首先提供几张基本表: weibo weibo_index user user_index follow fans id id id id id id content uid name gender uid uid image time avatar geo follow_id fans_id geo signature create_at create_at 这里之所以把follow和fans表拆开主要是方便后续分表，因为关注关系还是量很大的，我们可以将follow表和fans表根据uid进行hash分表，当然这里weibo表和user表也是可以做分表的，不过因为weibo表和user表不过分表维度并不好找，比如user表可以根据uid，根据注册时间，根据地理位置信息等维度分表，所以这里直接为两张表开一张index表，他们之间id完全相同，里面的维度字段都做索引，每次查找可以先在index表里搜索，拿到id后再去另一张表获取信息。 拉模式 拉模式用时间换区空间，不存储额外的信息，也是最简单的一种方案,因此就不拆开分析了，weibo/create和weibo/delete都是对于weibo,weibo_index表的增删，follow/create和follow/delete也只是增删follow和fans表内容，稍微复杂一点就是user/feedline，在获取feed列表的时候，首先从follow表中获得当前用户关注所有人的uid，然后在weibo_index表中，筛选uid在他关注列表的weibo_id，再从weibo表中获取数据，再将结果返回。 拉模式的思路简单明了，但是相对来说，用户在拉取feedline的时候代价很大，需要多次读取数据才能拿到想要的数据信息。 推模式 今天着重分析推模式的整体过程，首先由于是推模式，新增一张feed表,包括id，uid，weibo_id三个字段，这样简化了拉模式中获取feedline的复杂逻辑，但是在其他四个操作的时候都要做相应的操作。 follow/create（A关注B） 首先写入follow和fans表 将B发布的所有weibo_id塞入A的feed表 follow/delete（A取关B） 删除follow和fans表的记录 将A的feed表中所有是B发布的的weibo_id删除 weibo/create（A关注B，B发布了微博） 将新发布的内容写入weibo和weibo_index表中 将weibo_id塞入B所有粉丝的feed列表中 weibo/delete（A关注B，B删除了微博） 将weibo和weibo_index表中的内容删掉 将B所有粉丝的feed列表中的feed记录删除 本篇仅在分析推拉模式中的逻辑操作，下一篇将着重分析，分表，缓存，队列等方案在推模式中的具体实现。由于拉模式相对简单，就不再讨论了。","tags":[{"name":"架构","slug":"架构","permalink":"http://yunxuan.site/tags/架构/"},{"name":"Feed","slug":"Feed","permalink":"http://yunxuan.site/tags/Feed/"}]},{"title":"新兵训练之Feed架构设计（一）","date":"2016-09-09T08:49:29.000Z","path":"新兵训练之Feed架构设计（一）/","text":"这个系列主要是分享在新兵训练营学到的关于feed架构的设计，之间包括很多的内容，包括分库，分表，队列，缓存，主从等内容。 什么是Feed架构 既然是做feed架构的设计，那么第一件事就是解释什么是feed。简单来说，就是用户之间可以关注，用户可以发布信息，而关注他的人就可以看到他的动态。微博，twitter，facebook，微信朋友圈等都是典型的feed。 Feed架构的两种模式 这里先不考虑底层的数据库缓存队列如何用，先考虑整体的设计方案，Feed架构通常有两种模式，推模式和拉模式。 推模式 推模式顾名思义，即PUSH。可以理解为每一个用户都有两个信箱，一个发件箱，一个收件箱。当一个用户发布一条微博时，会将发布的微博放到他自己的发件箱，然后系统做处理，将这封微博复制N多份，分别投递到所有的关注了他的用户的收件箱中。然后当一个用户想要查看好友动态的时候，直接从自己的收件箱里把自己收到的消息按照时间排序显示出来就可以了。 我们可以对推模式进行一些探讨，推模式其实有个很明显的特点，那就是用空间来换取拉取feed的时间。为了保证每个用户都有收件箱和发件箱，就要有很大的存储空间来存储feed表。而且推模式本身也会出现问题，那就是对于系统处理的压力会增大，比如一个大V他有3000万的粉丝，那么他发送了一条微博，如果将这条消息，复制3000w份分发出去，仍然是一个很大的量，对于这列这叫应用，虽然不会要求延迟一定很小，但是对于这种粉丝超级多的大V推模式显然不是一个好的方式。 拉模式 拉模式就相对来说简单了，就是当一个用户需要获取feed流的时候，先获得他的关注列表，再找出微博创建人在这个列表内的微博，然后按照时间降序取就可以了，相比来说拉模式的逻辑更加简单一些了。 拉模式与推模式正好相反，就是利用时间来换取空间，用户在拉取feed的时候需要先获取用户的关注列表，再取查这些人发布的微博，获得最新的feed页面。那么如何保证响应时间，能够保证在一定时间为完成这个过程。这种模式就适合关注数并不是很多的那种，正好能够解决推模式中无法很好解决的粉丝过多的问题。 推拉模式的对比 情况 推模式 拉模式 获取Feed 简单高效 实时计算量大，与关注数量相关 发布Feed 要推送给所有粉丝 不需要推送 好友关系变更 关注和取关都需要feed变更 不需要处理 关注多粉丝少 合适 不合适 关注少粉丝多 不合适 合适 关注多粉丝多 ？ ？ 其实可以看出，推模式和拉模式各有各的优势和劣势，所以更加合适的方法应该是推拉结合，那么这样的话问题就来了，边界值如何设置，到底什么时候该推什么时候该拉？ 目前业界的一些业务实践业务形态对比 社交关系 关注数 粉丝数 Twiteer 公开、弱关系 上线5000 没有限制 Weibo 公开、弱关系 上线3000 没有限制 WeChat 私密、强关系 上限5000 上限5000 工程实践–Twitter 普通用户基于推方式+ 核心关注点为怎么尽快推送给关注者+ 大V用户基于拉方式 工程实践-Weibo 公开性微博基于拉方式+ 核心关注点为怎么提高聚合效率,包括响应时间和成本+ 私密性微博基于推方式 工程实践-WeChat 完全基于推模式","tags":[{"name":"架构","slug":"架构","permalink":"http://yunxuan.site/tags/架构/"},{"name":"Feed","slug":"Feed","permalink":"http://yunxuan.site/tags/Feed/"}]},{"title":"程序猿的职场经验（一）","date":"2016-09-09T05:52:16.000Z","path":"程序猿的职场经验（一）/","text":"在从学校毕业之前，一直觉着程序猿懂技术有能力就差不多了。感觉像电视剧里那种职场斗争估计我这辈子都不可能碰上了。不过等真的进了公司，发现自己还是太天真了，虽然不是像电视剧里那种勾心斗角，但是确实还有一些经验要学习，所以就写这么一个分类，算是记录一下以后学到的不属于技术分享但也很重要的东西。 还是要从上次出了线上bug说起，虽然是个小问题，而且也并不是我的锅，可还是让我有了一点阴影，之后每次推代码都小心谨慎。虽然小心一点本身也没有错，但是感觉有点过了。一个同事看我貌似是因为上次那个事情有阴影了，就开始跟我讲了一堆来开导我，还是很感谢他的。在这里也总结一下，也希望能帮助刚进入工作岗位的程序猿吧。 别怕踩坑首先就是作为技术人员，踩坑并不可怕，应该庆幸这次遗留的问题暴露出来了，如果没有暴露，说不定以后会有更大的问题。对于踩坑，更能重要的是找到问题的原因，及时填坑，总结，避免再出现类似的问题，这样的踩坑也才有价值。 别心眼太小线上出现问题是比较严重的情况。但是试问哪个程序员没出过线上问题，及时解决问题，总结经验才是更重要的。细心谨慎但也别杯弓蛇影。前几天在掘金上还看到这个讨论，工作中，你捅过多大的篓子,里面也是诸多神人，譬如直接在生产环境运行这个语句： 1$ sudo rm -rf / BOOMBOOMBOOM，咖喱给给！ 论队友的重要性有个好的队友是非常重要的，有个猪队友会被坑的很惨，出了问题乱甩锅，做事不认真之类的。不过好在我现在的队友还是可以的，除了有点马虎，整天扯淡，还算是不错了。 高调做事低调做人之前在知乎看到的对于这句话理解的讨论，原地址贴在这里。（侵权删） 复杂的事情简单做，简单的事情认真做，认真的事情重复做，这就是高调做事了 有了成绩不要骄傲，这就是低调做人了","tags":[{"name":"职场经验","slug":"职场经验","permalink":"http://yunxuan.site/tags/职场经验/"}]},{"title":"从一次踩坑说起","date":"2016-09-08T05:54:04.000Z","path":"从一次踩坑说起/","text":"粗略算起来，从毕业入职到现在也已经两个多月了，这两个月来，算是逐渐熟悉了在公司的生活。还是很庆幸同事还有老大都还挺随和的，每天跟他们扯扯淡，搬搬砖，也还不错。不过前一段时间踩到了一个坑，就在这记录一下，毕竟是工作后第一次踩这种坑，希望能总结一下。 接到的是一个SEO的小需求，就要在某个页面，加上个友情链接，原有的友情链接的逻辑是保存在数据库里，然后mc里缓存一个小时。其中不同的页面有不同的type_id，根据type_id进行分类，其中比较特殊的是首页，type_id是0。\u0010因为是给一个新的页面加入友情链接，其实给这个页面分配一个type_id就好了。但是上线那天晚上就出现了一个很怪的情况，首页的友情链接里莫名其妙的混进了很多其他页面的友情链接，因为是第一次遇到这种线上的bug，其实是有点慌，赶紧看看是哪来的这些东西。查了一下发现是缓存里面的数据出错了，数据库里是正确的，就先清空了缓存，恢复了线上，开始走查代码是怎么回事。 结果真的发现了一个巨坑，原来后台编辑或者添加友情链接的时候，并不是直接从库里读取，而是手动去更新缓存。虽然本身更新缓存的逻辑没问题，但是这仅限于当时没有主从库的时候。当时写主读从的时候，后台用户添加了一条新的友情链接，然后直接开始编辑，编辑的逻辑是读数据然后修改数据库，然后改写缓存，但是当刚刚添加立即编辑时，因为主从库延迟，从库里根本读不到，然后他没有设置合法性判断，直接取其中的type_id强制转换成int，所以悲剧的null转成了0，就写入了首页的缓存，😔都是泪啊，当然解决办法也很简单，加了合法性判断，编辑时没有读到数据时直接return false。 事后想想，虽然这个线上bug跟我半毛钱关系都没有，不过还是要写了一个总结提交给老大，也是对于一次线上问题的完整总结吧，大概包括以下几点： 故障时间和描述 定位bug，查询出故障原因 故障的解决方案 改进方案和总结 以后出了问题自己也要这样总结吧，一定把这些总结落实到文档中，记录下来，毕竟踩坑是为了以后能不再掉进去，总结经验才能不断进步嘛。","tags":[{"name":"踩坑","slug":"踩坑","permalink":"http://yunxuan.site/tags/踩坑/"},{"name":"线上问题处理","slug":"线上问题处理","permalink":"http://yunxuan.site/tags/线上问题处理/"}]},{"title":"Hello World","date":"2016-09-07T04:52:14.000Z","path":"hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]